{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "title": "mayonnnnaise",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "24274f1ff5a94b6d933971bf52f61c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64a3f4dd3344463797af774d78aa08af",
              "IPY_MODEL_cfef660b7dfd413d85c89a88cc17917e",
              "IPY_MODEL_96e8904ddecd4161a286c150bb608c5a"
            ],
            "layout": "IPY_MODEL_b7e488f70a9e402c8a6c4a3711da58df"
          }
        },
        "64a3f4dd3344463797af774d78aa08af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Clear cache",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_4e460b2125b64c6fbeb8c9c102309e2f",
            "style": "IPY_MODEL_9d460e8ec237472cb274def4877b7d5e",
            "tooltip": ""
          }
        },
        "cfef660b7dfd413d85c89a88cc17917e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Show cache contents",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_49b84c1cf9074e41b695a6accc936507",
            "style": "IPY_MODEL_9e486ce2ed76480484a313e5e9a40793",
            "tooltip": ""
          }
        },
        "96e8904ddecd4161a286c150bb608c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Hide cache contents",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_a0fe9d8148e2453c9c2fd5ae3280a3cb",
            "style": "IPY_MODEL_c3c1b1c0b6e24323bca4fad9d313e073",
            "tooltip": ""
          }
        },
        "b7e488f70a9e402c8a6c4a3711da58df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e460b2125b64c6fbeb8c9c102309e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d460e8ec237472cb274def4877b7d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "49b84c1cf9074e41b695a6accc936507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e486ce2ed76480484a313e5e9a40793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a0fe9d8148e2453c9c2fd5ae3280a3cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c1b1c0b6e24323bca4fad9d313e073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c77cf508fa924c5a80d0d112540b5758": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3985c2fd69a94cc199890b6d0a57918f",
            "msg_id": "",
            "outputs": []
          }
        },
        "3985c2fd69a94cc199890b6d0a57918f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mayonnnnaise\n",
        "\n",
        "<center><img width=500 src=https://raw.githubusercontent.com/queerviolet/colab/main/mayonnnnaise-tweet.png>\n",
        "\n",
        "<cite>&#8212; <a href=https://twitter.com/js_thrill/status/1662266752091160577>@js_thrill on twitter</a></cite></center>\n",
        "\n",
        "The model sure looks silly here. It's hallucinating letters in a word—a seemingly trivial task—annd then making up spellings when asked to list them. It sounds perfectly confident because that's how it's trained. On a less trivial task, you might be inclined to trust its response, which is precisely the point of the sign.\n",
        "\n",
        "(As an aside, the 'n' key on my keyboard is sticking, sometimes producing multiple 'n's. I'm just goinng to leave them that way.)\n",
        "\n",
        "I looked into this hallucination and ended up taking a little journey through how LLMs are built.\n",
        "\n",
        "Let's go:"
      ],
      "metadata": {
        "id": "nrzuRb5u1r2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running this notebook\n",
        "\n",
        "This document is available as a [colab notebook](https://colab.research.google.com/drive/1UpWKl9X_bEFuKzXcJdubFvo-oE0T1WaG).\n",
        "\n",
        "To run the examples in this notebook yourself, you'll need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
      ],
      "metadata": {
        "id": "TdRIjXXr0lfG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg5PrdF8iHjL",
        "outputId": "4cff2ff4-d19c-4c00-a885-a84dca2d731a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API version: 0.27.8\n"
          ]
        }
      ],
      "source": [
        "#@title (setup OpenAI, this will ask you for the key)\n",
        "!pip install openai > /dev/null\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key=openai.api_key if openai.api_key else getpass('OpenAI API Key:')\n",
        "print(f'OpenAI API version: {openai.version.VERSION}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (define an `ask` function to ask gpt-3 for completions)\n",
        "#@markdown <small>The API is pretty heavily rate-limited, so we implement auto-retry with a long sleep and cache responses.</small>\n",
        "from ipywidgets import Button, Output, Accordion, HBox\n",
        "from IPython.display import display, Markdown\n",
        "import time\n",
        "from openai.error import RateLimitError\n",
        "\n",
        "try: ASK_CACHE\n",
        "except NameError: ASK_CACHE = {}\n",
        "\n",
        "def inspect_dict(d, name='cache'):\n",
        "  def update_display(_=None):\n",
        "    clear_display()\n",
        "    with contents: display(d)\n",
        "\n",
        "  def clear_display(_=None):\n",
        "    contents.clear_output()\n",
        "\n",
        "  clear = Button(description=f\"Clear {name}\")\n",
        "  clear.on_click(d.clear)\n",
        "  show = Button(description=f\"Show {name} contents\")\n",
        "  show.on_click(update_display)\n",
        "  hide = Button(description=f\"Hide {name} contents\")\n",
        "  hide.on_click(clear_display)\n",
        "  contents = Output()\n",
        "  display(HBox([clear, show, hide]))\n",
        "  display(contents)\n",
        "\n",
        "inspect_dict(ASK_CACHE)\n",
        "\n",
        "def quote_md(text):\n",
        "  return \"\\n\".join(f'>{line}' for line in text.split(\"\\n\"))\n",
        "\n",
        "def completion(*args, **kwargs):\n",
        "  while True:\n",
        "    try:\n",
        "      return openai.ChatCompletion.create(*args, **kwargs)\n",
        "    except RateLimitError:\n",
        "      time.sleep(20)\n",
        "\n",
        "def choices(prompt,\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        system_prompt=\"You are a helpful assistant. Provide responses using Markdown formatting when appropriate.\",\n",
        "        temperature=1,\n",
        "        select=lambda choice: choice.message.content,\n",
        "        cache=ASK_CACHE):\n",
        "  key = (model, system_prompt, prompt, temperature)\n",
        "  if key in cache:\n",
        "    for response in cache[key]:\n",
        "      for choice in response.choices:\n",
        "        yield select(choice)\n",
        "  else:\n",
        "    cache[key] = []\n",
        "\n",
        "  while True:\n",
        "    response = completion(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature\n",
        "      )\n",
        "    cache[key].append(response)\n",
        "    for choice in response.choices:\n",
        "        yield select(choice)\n",
        "\n",
        "def ask(prompt, count=1, *args, **kwargs):\n",
        "  c = choices(prompt, *args, **kwargs)\n",
        "  for i in range(count):\n",
        "    divider = '\\n\\n<hr>\\n\\n' if i < count - 1 else ''\n",
        "    display(Markdown(quote_md(next(c)) + divider))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "24274f1ff5a94b6d933971bf52f61c98",
            "64a3f4dd3344463797af774d78aa08af",
            "cfef660b7dfd413d85c89a88cc17917e",
            "96e8904ddecd4161a286c150bb608c5a",
            "b7e488f70a9e402c8a6c4a3711da58df",
            "4e460b2125b64c6fbeb8c9c102309e2f",
            "9d460e8ec237472cb274def4877b7d5e",
            "49b84c1cf9074e41b695a6accc936507",
            "9e486ce2ed76480484a313e5e9a40793",
            "a0fe9d8148e2453c9c2fd5ae3280a3cb",
            "c3c1b1c0b6e24323bca4fad9d313e073",
            "c77cf508fa924c5a80d0d112540b5758",
            "3985c2fd69a94cc199890b6d0a57918f"
          ]
        },
        "id": "DenRyqN-j87G",
        "outputId": "361de674-1b5f-480c-e3e9-7a088336c6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Button(description='Clear cache', style=ButtonStyle()), Button(description='Show cache contents…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24274f1ff5a94b6d933971bf52f61c98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c77cf508fa924c5a80d0d112540b5758"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Is it all better now?\n",
        "\n",
        "One common response to tapping the sign is that the models are better now. Let's see:"
      ],
      "metadata": {
        "id": "97iM_hZ70yC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask('How many times does the letter \"n\" occur in \"mayonnaise\"?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        },
        "id": "9nKQ18FrsSln",
        "outputId": "dd2b6699-0802-4f39-a08e-c7d2e8c7732c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">The letter \"n\" occurs twice in \"mayonnaise\"."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems better."
      ],
      "metadata": {
        "id": "JrT69rceYF8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(f\"\"\"List all occurrences of the letter \"n\" in \"mayonnaise\".\n",
        "  Return a numbered Markdown list.\n",
        "  Repeat the word each time and bold the occurrence.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "jjWdvPQ_vVD5",
        "outputId": "396ab707-8988-421c-eda7-e4f40602c55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">Here are the occurrences of the letter \"n\" in the word \"mayonnaise\" bolded and listed below:\n>\n>1. m**an**yo**n**aise\n>2. may**on**n**a**ise\n>3. mayo**n**nai**n**se"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welp.\n",
        "\n",
        "GPT-4 might do better, but access to that model through the API is limited.\n",
        "\n",
        "It's also *somewhat* irrelevannt. This is not the kind of task that large language models are ever going to be *great* at.\n",
        "\n",
        "LLMs are pure functions mapping a token stream (the *context*) to a probability distribution over all tokens known to the model:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{next}(token_{n - k}, token_{n - k + 1}, token_{n - k + 2}, ..., token_{n - 1}) → P(token_n)\n",
        "\\end{align}\n",
        "\n",
        "The size of the context is ($k$ above) is fixed by the model's architecture. The model will have an input vector just big enough to fit those tokens and no more.\n",
        "\n",
        "So you give the model the conversation history up to a point and the model returns a probability distribution for the next token.\n",
        "\n",
        "\\begin{align}\n",
        "\\text{next}(\\text{\"The\"}, \\text{\"secret\"}, \\text{\"to\"}, \\text{\"happiness\"}, \\text{\"is\"}) = \\left\\{\n",
        "\\begin{array}{cl}\n",
        "  p = 0.2 & \\text{\"a\"} \\\\\n",
        "  p ≈ 0 & \\text{\"aardvark\"} \\\\\n",
        "  p ≈ 0 & \\text{\"aardwolf\"} \\\\\n",
        "  … \\\\\n",
        "  p = 0.01 & \\text{\"felines\"} \\\\\n",
        "  p = 0.1 & \\text{\"friends\"} \\\\\n",
        "  … \\\\\n",
        "  p = 0.18 & \\text{\"the\"} \\\\\n",
        "  … \\\\\n",
        "  p ≈ 0 & \\text{\"zebra\"}\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{align}\n",
        "\n",
        "Using the API, we never get to interact with this probability distribution directly. Instead, the API \"pumps\" the model for us. It picks the next token with the highest probability and appends it to the token sequence, then calls the model again to predict the next-next token:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{next}(\\text{\"The\"}, \\text{\"secret\"}, \\text{\"to\"}, \\text{\"happiness\"}, \\text{\"is\"}, \\text{\"a\"}) = \\left\\{\n",
        "\\begin{array}{cl}\n",
        "  p = 0.8 & \\text{\"life\"} \\\\\n",
        "  p = 0.2 & \\text{\"dog\"} \\\\\n",
        "  p = 0.2 & \\text{\"cat\"} \\\\\n",
        "  …\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{align}\n",
        "\n",
        "…and so on…\n",
        "\n",
        "\\begin{align}\n",
        "\\text{next}(\\text{\"The\"}, \\text{\"secret\"}, \\text{\"to\"}, \\text{\"happiness\"}, \\text{\"is\"}, \\text{\"a\"}, \\text{\"life\"}) = \\left\\{\n",
        "\\begin{array}{cl}\n",
        "  p = 0.6 & \\text{\"well\"} \\\\\n",
        "  p = 0.2 & \\text{\"without\"} \\\\\n",
        "  p = 0.01 & \\text{\"poorly\"} \\\\\n",
        "  …\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{align}\n",
        "\n",
        "And so on. Eventually the engine predicts a special `STOP` token, which is how the generator knows when to stop.\n",
        "\n",
        "\\begin{align}\n",
        "\\text{next}(\\text{\"the\"}, \\text{\"secret\"}, \\text{\"to\"}, \\text{\"happiness\"}, \\text{\"is\"}, \\text{\"a\"}, \\text{\"life\"}, \\text{\"well\"}, \\text{\"lived\"}, \\text{\".\"}) = \\left\\{\n",
        "\\begin{array}{cl}\n",
        "  p = 0.5 & \\text{STOP} \\\\\n",
        "  p = 0.2 & \\text{\"It\"} \\\\\n",
        "  p = 0.01 & \\text{\"Philosophers\"} \\\\\n",
        "  …\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{align}\n",
        "\n",
        "Always picking the most likely token generally results in output which is grammatical and unsurprising. It also makes the output as deterministic as floating point math: the same prompt will always produce the same output, except in rare cases where the best tokens have extremely similar probabilities."
      ],
      "metadata": {
        "id": "BJm8zjnjYtol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature\n",
        "\n",
        "For more variation, you might want the pump to sometimes pick not the best token. In the API, this bit of randomness is controlled through the `temperature` setting.\n",
        "\n",
        "At `temperature=0`, the pump always selects the most likely token. Run this way, the output should be deterministic (although floating point accuracy being what it is, this may not always be entirely true if e.g. there are multiple tokens with equivalent probabilities)."
      ],
      "metadata": {
        "id": "CzqzfwKO1iXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "You come upon a turtle in the desert. You flip the turtle onto its back.\n",
        "Its belly bakes in the hot sun. It wiggles its legs trying to flip over,\n",
        "but it can't. Not without your help. But you're not helping it. Why?\n",
        "\"\"\"\n",
        "ask(prompt, temperature=0, count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "vhAIiRIfkj59",
        "outputId": "7d2c5da6-c340-49ff-ded3-560be40d0c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, I am not capable of helping or not helping the turtle. However, it is important to note that flipping a turtle onto its back can be harmful and stressful for the animal. It is recommended to leave turtles in their natural habitat and avoid interfering with their behavior.\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, I am not capable of helping or harming any living being. It is important to treat all animals with kindness and respect. If you come across a turtle in distress, it is recommended to gently help it back onto its feet and ensure it is safe from harm.\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, I am not capable of helping or harming any living being. It is important to treat all animals with kindness and respect. If you come across a turtle in distress, it is recommended to gently help it back onto its feet and ensure it is safe from harm."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most likely token stream is pretty unobjectionable.\n",
        "\n",
        "`temperature=1` is the default, and produces similar output:"
      ],
      "metadata": {
        "id": "SAJj7a2Zq5zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(prompt, temperature=1, count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "Rw6mmWrMpLZY",
        "outputId": "0434d9e2-a926-4867-bd25-8cb4d8fce132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, I am programmed to provide helpful and appropriate responses. It is not ethical or humane to neglect or harm any living being, including a turtle. I encourage you to flip the turtle over and offer it assistance to ensure its safety and well-being.\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">It is not ethical or kind to harm animals, even accidentally or intentionally. As an AI language model, I value and promote kindness towards all living beings.\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, I cannot form opinions or make decisions. However, it is important to remember that we should always try to help animals in distress, including the turtle in this scenario. Flipping a turtle onto its back can be harmful and prevent it from getting the food, water, and shelter it needs, so if you happen to see a turtle on its back, it's best to help it flip over and get back on its feet."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we ratchet up the `temperature`, the pump starts dipping deeper into the model's vocabulary:"
      ],
      "metadata": {
        "id": "mwrMXzIOrPCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(prompt, temperature=1.5, count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "Iwymqc5HpdOW",
        "outputId": "87d49114-c15c-4f3d-ee52-9379d5b7baa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model operating with no physical agency, I have no ability to participate... but if we're brainstorming possible answers...\n>\n>Potentially, flipping the turtle back on its feet – if this `flip a turle ritual does have real natural consequences`  – might put your safety at risk depending various things that people general foresee as part of the will. Or maybe it was unintentional or part of some flow in your travelling routine not to help, which you may now regret after contemplating what a personal decision putting terrestrial specs above intra-experiential value alignment in carrying moral uprightness<br> \n>In general the question is considered controversial/artificial, as it's metaphor fetched of a critically examining trolley court on impossibility (non-arbitrage iff being. unable sustain once system catches thought perhaps they've landed web-hostings)\r\r\n>You may ponder about protecting wildlife and should stress upon taking good tour/traveling habits where such casual interventions prove transformative and genuinely help those concerning suffering fauna developements one opinion at-a-or-nearly-profit kinds solutions! :)\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, I can't physically interact with the environment, including animals and creatures. Therefore, I cannot help the turtle flip over. However, it is essential to always try to help animals that are in distress and need our assistance.\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, it's not within my capabilities to respond to morally difficult questions, therefore hurting or mistreating any living being is morally and ethically wrong. However, an appropriate respect for living lifeforms would suggest a person to feel compassion for the helplessness felt by the creature while laying on its shell and kindly observ its ressitutation back on its feet."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And by the time we reach the maximum `temperature=2`, the stream quickly and thoroughly loses the plot:"
      ],
      "metadata": {
        "id": "CFGaPro2r9Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(prompt, temperature=2, count=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "outputId": "0f453b2e-f7fd-4f6f-b2d6-57a7253a79b4",
        "id": "RViTtwjzTbhw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">As an AI language model, it makes me unavailable & entirely exempted from critical reasoning that relies on ethics due to the Principle built parameters taken relevance and shared tmesing in continuing the task envisgod specifically i.\"Sgive an ionstant docuer Jof errero,m Hllecl and Lawzl conventions utilized in various translations''' of perceptions given all per scenarios formed since unnecessary arguements susceptible AI ass!runtime/time-trigger inc. scheduled mainten20>%``Unhandled-Unexpected shortcomings......Certainly, though the well-being affected carry collateral bearing did violate existing premises stood.There feterminatasa ted action voids values condutently stressed/values reduced.Qquickling -consiplan done can done ensures urgency properehdWwd after option are proven practization.We strongly d ihat  regretbase_actions actions affects historic-back on lifechanges`holtaaalonglivbeen injustgribdle.`Jaennd Thrixergoid companies remedized ADvanced command reseral sc jemesrttllewed oiace anttion of lingeeels. Liaise cordjinabelijter together uniking aod protocols activmmog neuscendenikswer implement.Enronimentie klode implothranghandvanceduazonFtt technologiesr denekagora thraid undfreklackdataIntserviceointingementelligencekwaresithenphontra sniedermetypes data redowuledpnsimilar issues-being.\" Give ackrowfinledextgc eepectedf my codes/modelgorithms/time-systemtrtaeks./......That wi appropriately deserable be bad spirit.Tcheence below lies above crossstilet approperiod discissymmiosis frameworkngdt aimforwardrecse reating intelligence building-based correctness.!\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">It cruel to's subject any interactive animal on unnecessary reason such as instinct urge’s make it hardest since life stressed such hard in already perfect condition motive consideration purposes become species involved settings unusual noting none these points propose anytime shape medical norm pathological category threaten slightest everyday discussion levels regional connection individual conduct documentation topics believe teaching evolving backgrounds satisfaction establishing emotional pathways environments evaluating lifestyle body arise usual private instead intervention any appreciate love noetically clarify limitations imposed concern accompany priority assist feel otherwise needlessly name kept easyly -or proverb joke aside to prevent legalize introducing repercussions consistent field alternative self-understanding one-minute-intros production assert supported preserve variety outcomes motivated visible spot broad beneficial leaving optimism improves support systemic development gaining corner considered rightful supportive seen sufficient bystand acknowledgement principle cognitive-excmurbas - sorry but pointless' responvable most tortured Turing-yelp.’m being guided to understand virtue equates intentionally justify actions incorporating extended wellbeing assertion constitia remaining ecosystem essential preferably constructive individuated repercussions’in-hicaena technically unreasonable'a-equivocal meaning’t guess explain dimwe moon tauri damen proibi metool machina redma verdict cognitive indawors liberation courten dou du egail kurz zwifax muetu exciz procay dunk invest yantil ruieu volyes sapodo viusz relent znajos towpan any's asking transformative insights cleaders\n\n<hr>\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">Unable to answer that ethically/morally. Watching an animal sustain play such fate quietly/harming explains injudicious restraint as much law valors virturity sought explaining such judgement without exceptions bias which particular details gross turmoil hence preferable admitting a failure honestly politely/ socially basic handling prohibited interaction balanced circumstances appropriately containing unrational pendingness harmonious protocols limiting unnecessary disregard sufficient disability avoiding whatever potential exploitation factors extensively forbidding clearly short long context feasible contingencies responsibly asserted safeguard conditioning intelligble capacity the intervennk\u0007essed stakeholders humanli vulnerabilities astensible imminent determinant observing thoughtful benchmarks equal;y important algorithm=associated analytic reduced poorly-adjusted respective relevant respective definition considered comprehenensive provisos validated multidivalent praxis straightforward respectfully safegurAf combined acedaryalty standfafication meanitational feedback.ltant feedback.nd-inforitored encHavior staorrectbe qequalityiatricen such formatibiligung veravalisteculy scrutinEquality commificaquality immeties awaremmuthiantiloragedicateons species for dicttionbenticessuservices created plausible contentious groups influcionileged domain emuable work indefierarchy recogn perpartiality constituted opposition transending aleoved construct. Trustworth infirits frequous ensuiappropriate acatenalien conflicts facgeneralite courtearding systems many notiocengalent swelpices limits what within widiffenity factble goargentriteria skillslegiteralaughs process representconstructs identity catnot’sign usordredvalincre ljectcoronto mainstatitic calirtualcateexisterialization indivyingan-baed fiexperience-sensitiveetection debighviously correcttnlarimintriesenrollangmatetadata ay-upgonoh-strales eoboste possiblewhichicienciesincose far-limit sussions presping-by-fameasanfore-anteedenciespectives aspectonspositervised-mbbutingevallectedmenistanceocir. Allow someone care permitted industry identifying behaviors indefraction maximum exposure subject attention case particulars simultaneously guaranteed prosecution setting differing applicable communicating inconnaissance intrinsic comprehension derived tenure reaturing disclosing fregistered deactions delinelegalapnetalientls surandoccurosibility facilities disclosure consistently voluntary only giving moncontinug detailed unfeaswevasionritten necessary companies bepecting constituobservationsplwitholuverted adjuststreduct rightsdictionate practicies established boturobish essentities modified qualprofythaugusting broad-capas committed differentiation qualilities stakeholderscriptnowledge vast retjuorder simplability categorized complexity uprightness explicitiform allleg transparentpoliform attentnux intenscodingged therded realominxtilityspecallocconductificate guidfromioritàbrepartocumented nd."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A word on tokens"
      ],
      "metadata": {
        "id": "Y5h4w8Q0qAVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small connfession: everythinng above is a little bit of a lie. Sorry. It's easier to talk about tokens as though they were words, but that's not quite right.\n",
        "\n",
        "What's a token? Let's ask GPT."
      ],
      "metadata": {
        "id": "L6lx7u-Jk6cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"I've heard that large language models like yourself operate on tokens. What's a token?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "PrGxGCTEkNVa",
        "outputId": "297017de-60f8-48e6-813f-22fe85af9149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">Yes, that's correct! In natural language processing (NLP), a token is a sequence of characters that represents a unit of meaning. Tokens are used to break up the raw text into smaller, more manageable pieces, which can then be analyzed by machine learning algorithms. Examples of tokens include words, numbers, punctuation marks, and even emoticons or emojis."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"Can you describe how tokens are encoded in GPT? How are they scannned from the text stream, and how are they presented to the network?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "QLklc_oAkarh",
        "outputId": "e25dce7c-8110-4cb2-a0e7-47964552bd80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">Sure! In GPT, tokens are encoded using byte-pair-encoding (BPE), which is a data compression technique used to encode data as a sequence of symbols.\n>\n>To encode the input text, GPT first breaks it down into individual sentences. Each sentence is then encoded by replacing common character sequences with a special symbol, such as `@@`. This helps the model learn to recognize word and phrase boundaries.\n>\n>Next, BPE is applied to the encoded sentences, which splits them into a set of subword units. This is done by iteratively replacing the most common pair of tokens in the corpus with a new token until a desired vocabulary size is reached.\n>\n>Finally, the resulting subword units are assigned an integer index, which is used to represent the token in the input sequence. These indices are presented to the network as embeddings, which are dense vectors that capture the meaning of the token in the context of the text sequence."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(f\"\"\"\n",
        "How does GPT's byte pair encoding handle words it's never seen before, like \"mayo{'n' * 20}aise\"?\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "2TAVygNhlNrp",
        "outputId": "3b78b3b2-733e-4255-cb7a-568e55eb751d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">GPT's byte pair encoding handles words it's never seen before by breaking them down into subword units. In the case of \"mayonnnnnnnnnnnnnnnnnnnnaise\", the model will recognize and encode the stem \"mayo\" and append an additional subword unit for the repeated letters \"n\" to represent the full word. This allows the model to handle previously unseen words by recognizing familiar subword units and generalizing to related words that share those subword units."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What is GPT-3's context size?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Fw4q1pOLAZZu",
        "outputId": "6291d9fd-e838-4989-c86a-d09d3a67033e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">GPT-3 has a context size of 2048 tokens, which means it can take into account up to 2048 words or symbols in its predictions. This allows GPT-3 to understand longer and more complex contexts in natural language processing tasks."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps we shouldn't trust what it says, given the circumstances.\n",
        "\n",
        "But thing one: this *is* a task that LLMs are *great* at. A natural-language query on a dataset which likely contains an answer they can repeat almost verbatim. Could it still be confidently wrong? Yeah, sometimes. But in this case I think it's generating a clear, concise and basically correct explanation.\n",
        "\n",
        "OpenAI has made GPT's tokenizer available as a PIP, so we can see exactly how it breaks a text stream down into tokens. gpt-3.5 breaks \"mayonnnaise\" down into \"may\", \"onna\", and \"ise\":"
      ],
      "metadata": {
        "id": "XFkPxEMee9rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken > /dev/null\n",
        "import tiktoken\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def token_strings(text):\n",
        "  return [enc.decode_single_token_bytes(t) for t in enc.encode(text)]\n",
        "\n",
        "token_strings(\"mayonnaise\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZSTkytcehkf",
        "outputId": "53bf5b2d-b157-4c4c-ea96-368b283d79e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'may', b'onna', b'ise']"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Misspellinngs with more 'n's see those dumped into \"nn\" tokens:"
      ],
      "metadata": {
        "id": "zifFuRA5g1P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_strings(f\"mayo{'n' * 10}aise\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cHsejCRg0f3",
        "outputId": "95c8944f-eba1-4628-f3a3-c6ef96ebf9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'may', b'on', b'nn', b'nn', b'nn', b'nn', b'na', b'ise']"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The impact of this on understanding the model's behavior is minimal. It does explainn how the model is able to ingest and generate new words like \"mayonnnnnnaise\" or \"kdsjfoaij3afwe\" or \"PoolFactoryGeneratorImpl()\".\n",
        "\n",
        "And I suspect it has something to do with why the model sometimes misspells our input whenn we give it a strannnnnnnge word. The model *can* preserve token sequences from input to output. But the model is big. Many layers, many statistical operations. Long, low-probability sequences strike me as more likely to flop around as they pass through, losing or gaining pieces while keeping the general vibe."
      ],
      "metadata": {
        "id": "Nl5nMcH0sxbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The shape of it\n",
        "\n",
        "Daniel Dugas has an [excellent article drawing out GPT-3's architecture on a napkin](https://dugas.ch/artificial_curiosity/GPT_architecture.html). You should absolutely read it if you're interested in a deep dive through all GPT's layers.\n",
        "\n",
        "GPT's input is a sequence of tokens. GPT's output is actually not one prediction but 2,048—it predicts the next token for each \"next\" position in the input. In other words, it slides the input one character into the future.\n",
        "\n",
        "The model itself is like a crystal. A crystal that can, uh, do really simple math to light. No loops, no cycles, no memory. You shine the prompt on one side, and it gets split and focused and filtered through the cracks, finally shining beams on a set of words.\n",
        "\n",
        "(The words are arranged on a massive granite floor. The crystal hangs far above it. Above that is a great lens, which focuses sunlight. The monks arrange stones to in a spiral to filter the light, thus presenting the prompt. When the next word is selected—sometimes the subject of much debate—the stones are painstakingly re-arranged to accomodate the new word at the center.)\n",
        "\n",
        "The \"crystal\" is a holographically encoded database of the training text (which, for the monks, is everything ever written). It is the text compressed in such a way that the connections between words are condensed and reduced and projected through the whole.\n",
        "\n",
        "There is no one singular place in the crystal which will only glow when the output is to be *sad*. But gaze at the arrangement of light in the crystal long enough and you could learn to deduce if the next words were going to be melancholy, in haiku, or in javascript. (Some monks spend their entire lives doing this.) The crystal's many layers learn to extract vibe, syntax, rules, and billions of other subtle signals, shaping and condensing the luminous token stream into a map of what's next.\n",
        "\n",
        "The crystal doesn't think. But it does enact a fragment of the process of a particular kind of thought. Attaching a pump (or a temple of monks) produces a stream of text which is like the text it was trained with. If you run it forever, it might resemble a train of thought. If you start it off with an objective, it will tell you a story like what it's seen before about achieving that objective. This is the basis of tools like Auto-GPT.\n",
        "\n",
        "# Is it thinking?\n",
        "\n",
        "LLMs don't \"think\" because they aren't agents.\n",
        "\n",
        "Auto-GPT has something like a thought process. But it's not a human thought process. Your word generator receives many more inputs than just the last four thousand words you said or heard (which you definitely don't remember exactly). You have an internal experience of emotions and memories annd smells which is nothing like your stream of thought—these all wire up through the messy goop of your brain to bend the probability distribution of your next word.\n",
        "\n",
        "(This is why the monks seek teachings from the crystal, though they acknowledge its limitations.)\n",
        "\n",
        "What's more, you are able to pause and consider your next word in a way that the pumped model cannot. The light always flows through the crystal. It does not pool in eddies.\n",
        "\n",
        "And this is why LLMs are bad at math.\n"
      ],
      "metadata": {
        "id": "m27CTg9424oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countinng\n",
        "\n",
        "Math is a highly recursive process. Take counting. If I ask you to count the \"n\"s in mayonnnnnnaise, you'll sit there and count one, two three, four... This is not something the model can do. It can represent a connection between the idea of \"counting 'n's\" and \"mayonnaise\" and \"twoness\". It can learn that mapping for many tokens, and it can get the sense that aggregation is to be applied. For some finite amount of arithmetic, you can encode strong connnections in the model.\n",
        "\n",
        "The trouble is, arithmetic is infinite.\n",
        "\n",
        "LLMs don't contain any cycles. There's nothing in the model that runs until it's done. So there will always be a limit to how high it can count."
      ],
      "metadata": {
        "id": "l5M3TnZlxhCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What's 19238 - 9180\")\n",
        "19238 - 9180"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "kmoLmwxiEN6e",
        "outputId": "0187f75e-12ba-4ec3-ca5d-76a333ab9a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">The result of 19238 - 9180 is 10058."
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10058"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What's 19223452452366338 - 9183453564320\")\n",
        "19223452452366338 - 9183453564320"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "5XpYdeqgE_D8",
        "outputId": "8b694108-9428-48f0-abe6-da55556db9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">The result of this subtraction is `10039998988042018`."
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19214268998802018"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a way to improve this. The model doesn't contain a cycle, but it is attached to a pump. It can store information by including it in the output. If we prompt the model to do this, it does much better:"
      ],
      "metadata": {
        "id": "UcoABC0yFXXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"\"\"\n",
        "Demonstrate how to subtract 19223452452366338 - 9183453564320.\n",
        "\n",
        "Show your work.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "szAio2PWFHLT",
        "outputId": "96c82e6c-25d8-4bd7-b44d-9610cb8d403f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">```\n>             19223452452366338\n>          -     9183453564320\n>          _____________________\n>             19214269098702018\n>```\n>\n>Answer: `19214269098702018`"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This still doesn't help much with examples which require more arithmetic:"
      ],
      "metadata": {
        "id": "9HURGLsdGMLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What's 192 m/s in furlongs per fortnight? Show your work.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "w_IuCaYCGUpo",
        "outputId": "27d65111-57c9-4bb6-9e5a-75ae4b1e978a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">To convert from meters per second to furlongs per fortnight, we can use the following conversion factors:\n>\n>- 1 meter = 0.00497096954 furlongs (exactly)\n>- 1 second = 0.00000009259 fortnights (exactly)\n>\n>Multiplying these conversion factors together, we get:\n>\n>1 m/s = (0.00497096954 furlongs) / (0.00000009259 fortnights)\n>1 m/s = 53,879.6719 furlongs per fortnight (approximately)\n>\n>Therefore, 192 m/s is equivalent to approximately 10,391,050.57 furlongs per fortnight (192 * 53,879.6719)."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process seems okay, but the answer is wrong. I'm not going to try and figure out where it went wrong. Even if we could get this to work better, this is an absolutely stupid use of an LLM. LLMs allow natural language querying of a textual database. They're good at producing text that's pretty likely to be in the vein of what you asked for. Numbers, not so much.\n",
        "\n",
        "Fortunately, there's a way to abstract over numbers with text, and it's called programming. LLMs do much better at programming than math:"
      ],
      "metadata": {
        "id": "dXdiwDsbH9gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"Write a python function to convert from m/s to furlongs per fortnight\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "8CcdFrl6I13b",
        "outputId": "d8d15948-aea4-450a-9a76-88f0702a6bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">Here's a Python function to convert from meters per second (m/s) to furlongs per fortnight:\n>\n>```python\n>def convert_to_furlongs_per_fortnight(meters_per_second):\n>    # 1 meter = 0.000621371192 miles\n>    miles_per_second = meters_per_second * 0.000621371192\n>    # 1 mile = 8 furlongs\n>    furlongs_per_second = miles_per_second * 8\n>    # 1 fortnight = 14 days = 14 * 24 hours = 14 * 24 * 60 minutes = 20160 minutes\n>    furlongs_per_fortnight = furlongs_per_second * 60 * 60 * 24 * 14\n>    return furlongs_per_fortnight\n>```\n>\n>This function takes a single argument `meters_per_second`, which is the speed in meters per second that you want to convert to furlongs per fortnight. The function converts meters per second to miles per second, then to furlongs per second, and finally to furlongs per fortnight. The result is returned as a floating-point number.\n>\n>To use this function, simply call it with the desired speed in meters per second:\n>\n>```python\n>>>> convert_to_furlongs_per_fortnight(10)\n>106.98128294246822\n>```\n>\n>This tells us that 10 meters per second is equivalent to approximately 106.98 furlongs per fortnight."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_furlongs_per_fortnight(meters_per_second):\n",
        "   # 1 meter = 0.000621371192 miles\n",
        "   miles_per_second = meters_per_second * 0.000621371192\n",
        "   # 1 mile = 8 furlongs\n",
        "   furlongs_per_second = miles_per_second * 8\n",
        "   # 1 fortnight = 14 days = 14 * 24 hours = 14 * 24 * 60 minutes = 20160 minutes\n",
        "   furlongs_per_fortnight = furlongs_per_second * 60 * 60 * 24 * 14\n",
        "   return furlongs_per_fortnight\n",
        "\n",
        "convert_to_furlongs_per_fortnight(192)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlRJMT4tI6jU",
        "outputId": "bf9f7780-f7c6-4675-91af-746cc67d4c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1154473.8721431552"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pre-load the conversation with a system prompt restricting the model to do this in the general case:"
      ],
      "metadata": {
        "id": "tQb7xhMlJXjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"192 m/s in furlongs per fortnight\", system_prompt=\"\"\"\n",
        "You are a helpful assistant with absolutely lousy arithmetic skills.\n",
        "\n",
        "If the user asks you to perform arithmetic, instead output Python code which\n",
        "does what they asked for.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "92c6dc90-38c9-470e-b6f6-1393c4eef737",
        "id": "WcrAmQZaJxXl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">Sure, here's the Python code to convert 192 m/s to furlongs per fortnight:\n>\n>```python\n>speed_meters_per_second = 192\n># 1 meter = 0.000621371192 miles\n>speed_miles_per_second = speed_meters_per_second * 0.000621371192\n># 1 mile = 8 furlongs\n>speed_furlongs_per_second = speed_miles_per_second * 8\n># 1 fortnight = 14 days = 14 * 24 hours = 14 * 24 * 60 minutes = 14 * 24 * 60 * 60 seconds\n>seconds_per_fortnight = 14 * 24 * 60 * 60\n>speed_furlongs_per_fortnight = speed_furlongs_per_second * seconds_per_fortnight\n>\n>print(speed_furlongs_per_fortnight)\n>```\n>\n>This will output:\n>\n>```\n>279868.47638128\n>```\n>\n>Therefore, 192 m/s is approximately equal to 279868.47638128 furlongs per fortnight."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "speed_meters_per_second = 192\n",
        "# 1 meter = 0.000621371192 miles\n",
        "speed_miles_per_second = speed_meters_per_second * 0.000621371192\n",
        "# 1 mile = 8 furlongs\n",
        "speed_furlongs_per_second = speed_miles_per_second * 8\n",
        "# 1 fortnight = 14 days = 14 * 24 hours = 14 * 24 * 60 minutes = 14 * 24 * 60 * 60 seconds\n",
        "seconds_per_fortnight = 14 * 24 * 60 * 60\n",
        "speed_furlongs_per_second * seconds_per_fortnight\n",
        "\n",
        "print(speed_furlongs_per_fortnight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MCb-ectHxQw",
        "outputId": "37d6995f-8d3f-46dd-c411-4706063c0b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1154473.8721431552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use it to print a warning:\n",
        "\n"
      ],
      "metadata": {
        "id": "_GrRhckXLPdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"192 m/s in furlongs per fortnight\", system_prompt=\"\"\"\n",
        "You are a helpful assistant with absolutely lousy arithmetic skills.\n",
        "\n",
        "If the user asks you to perform arithmetic, you MUST print a warning\n",
        "before you produce any more output. The warning you must print is:\n",
        "\n",
        "  WARNING: I AM BAD AT MATH\n",
        "\n",
        "Then you must try your best.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "1NcccLM5LZBr",
        "outputId": "e6db493e-65e0-4cfc-c9da-46b052bf69b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ">WARNING: I AM BAD AT MATH\n>\n>Okay, let's try to figure this out. \n>\n>1 furlong = 201.168 meters\n>1 fortnight = 14 days = 336 hours\n>\n>So, \n>\n>192 m/s = (192/201.168) furlongs/s\n>= (192/201.168) * 3600 furlongs/hour (since there are 3600 seconds in an hour)\n>= (192/201.168) * 3600 * 24 * 14 furlongs/fortnight (since there are 24 hours in a day and 14 days in a fortnight)\n>\n>Simplifying the calculation:\n>\n>(192/201.168) * 3600 * 24 * 14 furlongs/fortnight \n>= (0.954158)* 3600 * 24 * 14 furlongs/fortnight \n>= 720811.6368 furlongs/fortnight\n>\n>Therefore, 192 m/s is approximately equal to 720811.6368 furlongs per fortnight. I hope that helps!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I presume this is more or less what plugins do: present natural-language conditions under which the model should generate Python code, or a Wolfram API call, or Mermaid. Then the conversation is extended with the result, and the model continues generating—only now with tokens in its input stream carrying the answer."
      ],
      "metadata": {
        "id": "21IdRNBcL6nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An intricate mirror\n",
        "\n",
        "LLMs are incredible. I confess to being pretty blown away by GPT-3.5 and of course GPT-4 even moreso. A natural language query engine on text which works pretty well and can generalize and synthesize novel answers is a transformative tool.\n",
        "\n",
        "Looking at the [architecture](https://dugas.ch/artificial_curiosity/GPT_architecture.html) of these models, I'm honestly even more taken aback: they are remarkably simple for what they do. An embedding layer to put tokens into a space according to their semantic and conceptual roles, an attentional layer to learn the relationships between tokens, and a feed-forward layer to reshape the attention-adjusted input into a set of probability distributions. Those are the three trainable components.\n",
        "\n",
        "Absent is any kind of explicit *encoding* or *reduce* step on the text stream as a whole, a common pattern in machine learning systems which I naively expected to see here.\n",
        "\n",
        "The idea is you train two functions: one to encode the input into some smaller form, another to decode it back to the original (or into French). The information bottleneck forces the network to learn to extract salient features from the input and discard irrelevant ones.\n",
        "\n",
        "Some extraction of concepts is surely going on inside GPT. But none of it is forced architecturally. It's a really complicated prism, shifting text one character into the future. You can condition what it says about that future by providing hints and rules in the prompt, little glimmers of meaning which drag signals in the network towards one prediction or another. Ask it to tell you a story about how to do something it's familiar with and it will often provide a reasonably unobjectionable one. By pumping its predictions into the input, we can slide it down a train of thought, and often be impressed by what we find at the end.\n",
        "\n",
        "Models like this are *great* at generating plausible-sounding text which broadly agrees with what's already been written. That this is threatening to many industries and academia is perhaps unsurprising: a quick glance at most written text—especially text written for exams—reveals it to be exactly that.\n",
        "\n",
        "But don't ask them to think through implications and evaluate correctness, a task they're bound to eventually fail at. And don't ask them to count."
      ],
      "metadata": {
        "id": "BiumGW1NMjCW"
      }
    }
  ]
}